# Deploy on Kubernetes cluster

Deploy on-prem:

## Create cluster

Install Kubernetes on every node:


```bash
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl

sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```

### Turn off node swap

```bash
sudo swapoff -a
```

### On master node, run:

```bash

sudo kubeadm init --apiserver-advertise-address 192.168.80.XXX --apiserver-bind-port 8443 --pod-network-cidr=10.244.0.0/16

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/admin.conf
sudo chown $(id -u):$(id -g) $HOME/.kube/admin.conf

export KUBECONFIG=$HOME/.kube/admin.conf

# to deploy image on master nodes
kubectl taint nodes --all node-role.kubernetes.io/master-

# apply network 
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
```

To get the join command later, run:

```bash
kubeadm token create --print-join-command
```

### On worker nodes:

```bash
# join cluster
sudo kubeadm join 192.168.80.XXX:6443 --token 5qnzpu.vd511emvhamywh5j     --discovery-token-ca-cert-hash sha256:b20459538093d67621317379d76ee9d3323b8a12f08eeb999f92903dd7a04f52

```

## Install cuda stuffs. 

From https://www.tensorflow.org/install/gpu

```bash
# Add NVIDIA package repositories
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin
sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/ /"
sudo apt-get update

wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb

sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt-get update

# Install NVIDIA driver
sudo apt-get install --no-install-recommends nvidia-driver-450
# Reboot. Check that GPUs are visible using the command: nvidia-smi

wget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/libnvinfer7_7.1.3-1+cuda11.0_amd64.deb
sudo apt install ./libnvinfer7_7.1.3-1+cuda11.0_amd64.deb
sudo apt-get update

# Install development and runtime libraries (~4GB)
sudo apt-get install --no-install-recommends \
    cuda-11-0 \
    libcudnn8=8.0.4.30-1+cuda11.0  \
    libcudnn8-dev=8.0.4.30-1+cuda11.0

```

## Disable xorg server

```bash
# check all current nvidia packages
dpkg -l | grep nvidia

# then uninstall all 
sudo apt purge xorg-nvidia-...

```

Run `sudo nano /etc/X11/xorg.conf`, and apply the following:

```
# /ect/X11/xorg.conf

# nvidia-xconfig: X configuration file generated by nvidia-xconfig
# nvidia-xconfig:  version 460.39

Section "ServerLayout"
    Identifier     "Layout0"
    Screen      0  "Screen0"
    # if you have 2 screens
    Screen 1 "Screen1"
    Screen 2 "nvidia"
EndSection

Section "Monitor"
    Identifier     "Monitor0"
    VendorName     "Unknown"
    ModelName      "Unknown"
EndSection

# your second monitor
Section "Monitor"
    Identifier     "Monitor1"
    VendorName     "Unknown"
    ModelName      "Unknown"
EndSection

Section "Device"
    Identifier     "intel1"
    Driver         "intel"
    BusID          "PCI:0@0:2:0"
    Option "AccelMethod" "SNA"
    Option "TearFree" "true"
    Option "DRI" "3"
EndSection

# if you have 2 monitors
Section "Device"
    Identifier     "intel2"
    Driver         "intel"
    BusID          "PCI:0@0:2:0"
    Option "AccelMethod" "SNA"
    Option "TearFree" "true"
    Option "DRI" "3"
EndSection

Section "Device"
    Identifier     "nvidia"
    Driver         "nvidia"
    BusID          "PCI:1@0:0:0"
    Option "ConstrainCursor" "off"
EndSection

Section "Screen"
    Identifier     "Screen0"
    Device         "intel1"
    Monitor        "Monitor0"
    DefaultDepth    24
    SubSection     "Display"
EndSection

# for the second monitor
Section "Screen"
    Identifier     "Screen1"
    Device         "intel2"
    Monitor        "Monitor1"
    DefaultDepth    24
    SubSection     "Display"
        Depth       24
    EndSubSection
EndSection

Section "Screen"
    Identifier "nvidia"
    Device "nvidia"
    Option "AllowEmptyInitialConfiguration" "on"
    Option "IgnoreDisplayDevices" "CRT"
EndSection
```

Another example

```
Section "ServerLayout"
    Identifier     "Layout0"
    Screen      0  "Screen0"
    Screen 1 "Screen1"
    Screen 2 "nvidia"
EndSection

Section "Monitor"
    Identifier     "Monitor0"
    VendorName     "Unknown"
    ModelName      "Unknown"
EndSection

Section "Monitor"
    Identifier     "Monitor1"
    VendorName     "Unknown"
    ModelName      "Unknown"
EndSection

Section "Device"
    Identifier     "intel1"
    Driver         "intel"
    BusID          "PCI:0@0:2:0"
    Option "AccelMethod" "SNA"
    Option "TearFree" "true"
    Option "DRI" "3"
EndSection

Section "Device"
    Identifier     "intel2"
    Driver         "intel"
    BusID          "PCI:0@0:2:0"
    Option "AccelMethod" "SNA"
    Option "TearFree" "true"
    Option "DRI" "3"
EndSection


Section "Device"
    Identifier     "nvidia"
    Driver         "nvidia"
    BusID          "PCI:1@0:0:0"
    Option "ConstrainCursor" "off"
EndSection

Section "Screen"
    Identifier     "Screen0"
    Device         "intel1"
    Monitor        "Monitor0"
    DefaultDepth    24
    SubSection     "Display"
        Depth       24
    EndSubSection
EndSection

Section "Screen"
    Identifier     "Screen1"
    Device         "intel2"
    Monitor        "Monitor1"
    DefaultDepth    24
    SubSection     "Display"
        Depth       24
    EndSubSection
EndSection

Section "Screen"
    Identifier "nvidia"
    Device "nvidia"
    Option "AllowEmptyInitialConfiguration" "on"
    Option "IgnoreDisplayDevices" "CRT"
EndSection
```

Reboot, hope things still work :drooling_face: 

Next steps:

- Run `nvidia-smi`. You should see there is no task running. If there are any, find some other ways to remove them. Good luck!

- Install `nvidia-docker` runtime [NVIDIA/nvidia-docker](https://github.com/NVIDIA/nvidia-docker). Follow here https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker

- Use nvidia-docker as default runtime by running:

```bash
sudo nano /etc/docker/daemon.json

# add the following
{
    "default-runtime": "nvidia",
    "runtimes": {
        "nvidia": {
            "path": "nvidia-container-runtime",
            "runtimeArgs": []
        }
    }
}
```

- Install `nvidia-cuda-tool` for k8s: https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/:

```bash
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta4/nvidia-device-plugin.yml
```

- On GPU nodes, run docker ps, you should see a running docker with image `k8s_nvidia-device-plugin-ctr_nvidia-device-plugin-daemonset-fpxld_kube-system...` and see its log to check if it is working:

```bash
docker logs 8547afc94b61 # its ID

# should be something like this
2021/03/23 06:36:49 Loading NVML
2021/03/23 06:36:49 Starting FS watcher.
2021/03/23 06:36:49 Starting OS watcher.
2021/03/23 06:36:49 Retreiving plugins.
2021/03/23 06:36:49 Starting GRPC server for 'nvidia.com/gpu'
2021/03/23 06:36:49 Starting to serve 'nvidia.com/gpu' on /var/lib/kubelet/device-plugins/nvidia-gpu.sock
2021/03/23 06:36:49 Registered device plugin for 'nvidia.com/gpu' with Kubelet
```

## Create AWS authentication info to pull images from ECR

You should login to the aws cli tool first.

Run the script at `blueedge-manager/deployment/k8s/cre.sh`:

```bash
export KUBERNETES_REGISTRY=aws-ecr
export DOCKER_ECR=362085406009.dkr.ecr.us-east-2.amazonaws.com
export DOCKER_USERNAME=AWS
# export DOCKER_EMAIL=your@email.com
export DOCKER_SECRET=$(aws ecr get-login-password --region us-east-2 | sed -e 's/.*-p //' -e 's/ .*$//')

kubectl delete secrets ${KUBERNETES_REGISTRY} 2> /dev/null

kubectl create secret docker-registry ${KUBERNETES_REGISTRY} \
--docker-server=${DOCKER_ECR} \
--docker-username=${DOCKER_USERNAME} \
--docker-password=${DOCKER_SECRET}
# --docker-email=${DOCKER_EMAIL}
```

## Deploy the BlueEdge Manager:

```bash
cd blueedge-manager/deployment

# create the system communication secret
kubectl apply -f secret.yml

# deploy the manager
kubectl apply -f deployment.yml

# deploy the service
kubectl apply -f service.yml
```